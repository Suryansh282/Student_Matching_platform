# -*- coding: utf-8 -*-
"""KMeans_model.ipynb

This script trains a KMeans model on student data, performs clustering, and finds matching students based on input data.
Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NdtQgxHskYoeVWsczWuXAYcstjXB9xLO
"""

import json
import numpy as np
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.preprocessing import MultiLabelBinarizer, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.metrics import pairwise_distances
import joblib  # For saving the model

def load_json_data(file_path):
    """
    Load student data from a JSON file.

    Args:
        file_path (str): Path to the JSON file containing student data.

    Returns:
        list: List of student data as dictionaries.
    """
    with open(file_path, 'r') as file:
        data = json.load(file)
    return data

# Load student data from the JSON file
file_path = 'students_large_dataset.json'  # Replace with your file path
students = load_json_data(file_path)

# Convert JSON data to a pandas DataFrame
student_df = pd.DataFrame(students)

# Standardize subject names to lowercase for uniformity
student_df['subjects'] = student_df['subjects'].apply(lambda x: [s.lower() for s in x])

# Encode subjects using MultiLabelBinarizer
mlb = MultiLabelBinarizer()
subjects_encoded = mlb.fit_transform(student_df["subjects"])

# Calculate average study hours for each student
avg_study_hours = student_df["study_hours"].apply(np.mean).values.reshape(-1, 1)

# Encode goals using a predefined mapping (example mapping shown)
goal_mapping = {
    "Exam preparation": 0,
    "Assignments": 1,
    "Concept review": 2,
    "Project work": 3,
    "Homework": 4,
    "Practice": 5
}
goals_encoded = student_df["goals"].map(goal_mapping).values.reshape(-1, 1)

# Combine encoded subjects, average study hours, and goals for each student
features = np.hstack([subjects_encoded, avg_study_hours, goals_encoded])

# Impute missing values in the features using the mean of each column
imputer = SimpleImputer(strategy="mean")
features_imputed = imputer.fit_transform(features)

# Scale the imputed features to have zero mean and unit variance
scaler = StandardScaler()
features_scaled = scaler.fit_transform(features_imputed)

# Train the KMeans model to cluster students into 3 groups
kmeans = KMeans(n_clusters=3, random_state=42)
student_df["cluster"] = kmeans.fit_predict(features_scaled)

# Save the trained KMeans model, scaler, imputer, and MultiLabelBinarizer for later use
joblib.dump(kmeans, 'kmeans_model.pkl')
joblib.dump(scaler, 'scaler.pkl')
joblib.dump(imputer, 'imputer.pkl')
joblib.dump(mlb, 'mlb.pkl')

print("KMeans model, scaler, and MultiLabelBinarizer have been saved.")

def find_matching_students(new_student, student_df, kmeans, scaler, mlb, n_matches=3):
    """
    Find the top matching students for a new student based on their study profile.

    Args:
        new_student (dict): The new student's data (name, subjects, study hours, goals).
        student_df (pd.DataFrame): DataFrame containing the student data.
        kmeans (KMeans): Trained KMeans model to predict the cluster for the new student.
        scaler (StandardScaler): Pre-trained scaler to standardize the features.
        mlb (MultiLabelBinarizer): Pre-trained MultiLabelBinarizer to encode subjects.
        n_matches (int): The number of closest matching students to return (default is 3).

    Returns:
        pd.DataFrame: The `n_matches` closest students from the same cluster.
    """
    # Process new student data
    new_subjects = mlb.transform([[s.lower() for s in new_student["subjects"]]])[0]
    new_avg_hour = np.mean(new_student["study_hours"])
    new_goal_encoded = goal_mapping[new_student["goals"]]

    # Create a feature vector for the new student
    new_student_features = np.hstack([new_subjects, new_avg_hour, new_goal_encoded]).reshape(1, -1)
    new_student_features_scaled = scaler.transform(new_student_features)

    # Predict the cluster for the new student
    cluster = kmeans.predict(new_student_features_scaled)[0]

    # Filter the students to those in the same cluster
    cluster_students = student_df[student_df["cluster"] == cluster]
    cluster_features = features_scaled[student_df["cluster"] == cluster]

    # Calculate pairwise Euclidean distances to find the closest matches
    distances = pairwise_distances(new_student_features_scaled, cluster_features, metric="euclidean").flatten()
    closest_indices = distances.argsort()[:n_matches]

    # Return the closest matching students
    return cluster_students.iloc[closest_indices]

# Append a new student and find matching students
new_student = {
    "name": "KULDEEP",
    "subjects": ["math", "biology"],
    "study_hours": [12, 17],
    "goals": "Exam preparation"
}

# Load previously saved models and encoders
kmeans_model = joblib.load('kmeans_model.pkl')
scaler = joblib.load('scaler.pkl')
mlb = joblib.load('mlb.pkl')

# Find the top 5 matching students for the new student
matching_students = find_matching_students(new_student, student_df, kmeans_model, scaler, mlb, n_matches=5)

# Print the top matching students' details
print("Top matching students:")
print(matching_students[["name", "subjects", "study_hours", "goals"]])

# Append the new student to the dataset and save it back to the JSON file
students.append(new_student)
with open(file_path, 'w') as file:
    json.dump(students, file, indent=4)

# Reload data and prepare features for clustering
student_df = pd.DataFrame(students)
student_df['subjects'] = student_df['subjects'].apply(lambda x: [s.lower() for s in x])
subjects_encoded = mlb.fit_transform(student_df["subjects"])
avg_study_hours = student_df["study_hours"].apply(np.mean).values.reshape(-1, 1)
goals_encoded = student_df["goals"].map(goal_mapping).values.reshape(-1, 1)

# Impute and scale features
features = np.hstack([subjects_encoded, avg_study_hours, goals_encoded])
imputer = SimpleImputer(strategy="mean")
features_imputed = imputer.fit_transform(features)  # Impute NaNs

scaler = StandardScaler()
features_scaled = scaler.fit_transform(features_imputed)  # Scale the imputed features

# Train and save the updated KMeans model
kmeans = KMeans(n_clusters=3, random_state=42)
student_df["cluster"] = kmeans.fit_predict(features_scaled)
joblib.dump(kmeans, 'kmeans_model.pkl')
joblib.dump(scaler, 'scaler.pkl')
joblib.dump(imputer, 'imputer.pkl')
joblib.dump(mlb, 'mlb.pkl')

print("Retrained model and updated data have been saved.")
